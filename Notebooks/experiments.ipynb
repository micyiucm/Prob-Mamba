{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "repo_root = Path(\"/home/ubuntu/michael/MSc-Machine-Learning-Project\")\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from itertools import product \n",
    "from mamba_ssm import Mamba\n",
    "from prob_mamba.utils import arma_fit_forecast, garch_fit_forecast, create_sequences, make_loaders, num_epochs, run_experiment_block_param_validated, param_count\n",
    "from prob_mamba.eval import build_seq2seq_loaders, prob_head_params, build_prob_mamba_budget_fixed_n, train_prob_mamba_with_history, plot_nll_history, eval_prob_rmse_qlike_laststep\n",
    "from prob_mamba.models import ProbabilisticMamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27875902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "data_dir = repo_root / \"Datasets\" / \"Processed\"\n",
    "\n",
    "#Set seed for reproducibility\n",
    "seed = 21003415\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82af17",
   "metadata": {},
   "source": [
    "## Models to test:\n",
    "1. Econometric baseline ($\\text{ARMA}$ for mean, $\\text{GARCH }(1,1)$ for volatility)\n",
    "2. Simple RNN\n",
    "3. Vanilla Mamba\n",
    "4. Prob_mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31020bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Econometric baseline on NYSE data\n",
    "train_df = feather.read_feather(data_dir / \"cleaned_NYSE_train.feather\")\n",
    "val_df   = feather.read_feather(data_dir / \"cleaned_NYSE_val.feather\")\n",
    "test_df  = feather.read_feather(data_dir / \"cleaned_NYSE_test.feather\")\n",
    "# 1. ARMA mean\n",
    "order, arma_rmse, mu_fc, arma_fit = arma_fit_forecast(\n",
    "    train_df, val_df, test_df, target_col='y_next', return_fit=True\n",
    ")\n",
    "\n",
    "# 2 GARCH variance on ARMA residuals\n",
    "g_rmse, g_qlike, g_nll, mu_hat, var_hat = garch_fit_forecast(\n",
    "    train_df, val_df, test_df, target_col='y_next',\n",
    "    dist='normal', scale=100.0,\n",
    "    mu_trval=arma_fit.fittedvalues,  # train+val fitted mean\n",
    "    mu_test=mu_fc                    # test mean forecasts\n",
    ")\n",
    "\n",
    "# Results\n",
    "print(f\"Econometric baseline on NYSE data: RMSE={arma_rmse:.6f}, QLIKE={g_qlike:.6f}, NLL={g_nll:.6f}\")\n",
    "resid = pd.Series(arma_fit.resid, dtype=\"float64\").dropna().reset_index(drop=True)\n",
    "plt.plot(resid.values)\n",
    "plt.title(\"ARMA residuals on NYSE data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Econometric baseline on IXIC data\n",
    "train_df = feather.read_feather(data_dir / \"cleaned_IXIC_train.feather\")\n",
    "val_df   = feather.read_feather(data_dir / \"cleaned_IXIC_val.feather\")\n",
    "test_df  = feather.read_feather(data_dir / \"cleaned_IXIC_test.feather\")\n",
    "# 1. ARMA mean\n",
    "order, arma_rmse, mu_fc, arma_fit = arma_fit_forecast(\n",
    "    train_df, val_df, test_df, target_col='y_next', return_fit=True\n",
    ")\n",
    "\n",
    "# 2 GARCH variance on ARMA residuals\n",
    "g_rmse, g_qlike, g_nll, mu_hat, var_hat = garch_fit_forecast(\n",
    "    train_df, val_df, test_df, target_col='y_next',\n",
    "    dist='normal', scale=100.0,\n",
    "    mu_trval=arma_fit.fittedvalues,  # train+val fitted mean\n",
    "    mu_test=mu_fc                    # test mean forecasts\n",
    ")\n",
    "\n",
    "# Results\n",
    "print(f\"Econometric baseline on IXIC data: RMSE={arma_rmse:.6f}, QLIKE={g_qlike:.6f}, NLL={g_nll:.6f}\")\n",
    "resid = pd.Series(arma_fit.resid, dtype=\"float64\").dropna().reset_index(drop=True)\n",
    "plt.plot(resid.values)\n",
    "plt.title(\"ARMA residuals on IXIC data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Econometric baseline on BTC data\n",
    "train_df = feather.read_feather(data_dir / \"BTC_USDT-5m_train.feather\")\n",
    "val_df   = feather.read_feather(data_dir / \"BTC_USDT-5m_val.feather\")\n",
    "test_df  = feather.read_feather(data_dir / \"BTC_USDT-5m_test.feather\")\n",
    "# 1. ARMA mean\n",
    "order, arma_rmse, mu_fc, arma_fit = arma_fit_forecast(\n",
    "    train_df, val_df, test_df, target_col='y_next', return_fit=True\n",
    ")\n",
    "\n",
    "# 2 GARCH variance on ARMA residuals\n",
    "g_rmse, g_qlike, g_nll, mu_hat, var_hat = garch_fit_forecast(\n",
    "    train_df, val_df, test_df, target_col='y_next',\n",
    "    dist='normal', scale=1000,\n",
    "    mu_trval=arma_fit.fittedvalues,  # train+val fitted mean\n",
    "    mu_test=mu_fc                    # test mean forecasts\n",
    ")\n",
    "\n",
    "# Results\n",
    "print(f\"Econometric baseline on BTC data: RMSE={arma_rmse:.6f}, QLIKE={g_qlike:.6f}, NLL={g_nll:.6f}\")\n",
    "resid = pd.Series(arma_fit.resid, dtype=\"float64\").dropna().reset_index(drop=True)\n",
    "plt.plot(resid.values)\n",
    "plt.title(\"ARMA residuals on BTC data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f541b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuation\n",
    "    sequence_length_map = {\n",
    "        \"BTC_USDT-5m\": 288,\n",
    "        \"cleaned_IXIC\": 270,\n",
    "        \"cleaned_NYSE\": 270\n",
    "    }\n",
    "    target_column = 'y_next' # next-period log return\n",
    "    # Data to process\n",
    "    Arrow_files_prefix= [\n",
    "        'cleaned_IXIC',\n",
    "        'cleaned_NYSE',\n",
    "        'BTC_USDT-5m'\n",
    "    ]\n",
    "    # Initialise a dictionary to hold all the final data\n",
    "    all_processed_data = {}\n",
    "\n",
    "    for prefix in Arrow_files_prefix:\n",
    "        print(f\"\\n Creating Sequences for {prefix}:\")\n",
    "\n",
    "        # Step 1:  Load original data to calculate log returns\n",
    "        original_path = f\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/{prefix}.feather\"\n",
    "        original_df = feather.read_feather(original_path)\n",
    "        date_col = \"date\" if \"date\" in original_df.columns else \"Date\"\n",
    "        original_df[date_col] = pd.to_datetime(original_df[date_col])\n",
    "        original_df = original_df.sort_values(date_col).reset_index(drop=True)\n",
    "        price_col = \"Price\" if \"Price\" in original_df.columns else \"close\"\n",
    "        if price_col not in original_df.columns:\n",
    "            raise ValueError(\"Expected column 'Price' or 'close' in dataframe.\")\n",
    "        # sanity: no non-positive prices\n",
    "        if (original_df[price_col] <= 0).any():\n",
    "            raise ValueError(f\"Non-positive prices found in {price_col} — clean data first.\")\n",
    "        #Compute log_price, log_returns\n",
    "        original_df[\"_log_price\"] = np.log(original_df[price_col])\n",
    "        original_df[\"ret_t\"] = original_df[\"_log_price\"].diff() # today's log return\n",
    "        original_df[\"y_next\"] = original_df[\"_log_price\"].shift(-1) - original_df[\"_log_price\"] # next period log return\n",
    "        original_df = original_df.dropna(subset=[\"y_next\"]).reset_index(drop=True)\n",
    "        print(\"Computed log_price, ret_t and y_next inb original data.\")\n",
    "\n",
    "        # Step 2: Load the pre-processed data\n",
    "        train_df = feather.read_feather(f'{data_dir}/{prefix}_train.feather')\n",
    "        val_df = feather.read_feather(f'{data_dir}/{prefix}_val.feather')\n",
    "        test_df = feather.read_feather(f'{data_dir}/{prefix}_test.feather')\n",
    "        for df in [train_df, val_df, test_df]:\n",
    "            df[date_col] = pd.to_datetime(df[date_col])\n",
    "        print(\"Loaded pre-processed taining, validation, and test data\")\n",
    "\n",
    "        # Step 3: Define features to prevent data leakage\n",
    "        columns_to_drop = ['Date', 'Name', price_col, '_log_price', \"y_next\"]\n",
    "        feature_columns = [col for col in train_df.columns if col not in columns_to_drop and col != date_col]\n",
    "        print(f\"Using {len(feature_columns)} input features.\")\n",
    "\n",
    "        # Step 4: Set sequence length\n",
    "        default_sequence_length = 270\n",
    "        short_prefix = prefix.replace(\"cleaned_\", \"\")\n",
    "        seq_len = sequence_length_map.get(prefix, sequence_length_map.get(short_prefix, default_sequence_length))\n",
    "\n",
    "        # Step 5: Create windowed sequence \n",
    "        X_train, y_train = create_sequences (train_df, seq_len, feature_columns, target_column)\n",
    "        X_val, y_val = create_sequences(val_df, seq_len, feature_columns, target_column)\n",
    "        X_test, y_test = create_sequences(test_df, seq_len, feature_columns, target_column)\n",
    "\n",
    "        # Diagnostic prints\n",
    "        print(f\"---Shapes for {prefix}---\")\n",
    "        print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "        print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "        print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")    \n",
    "\n",
    "        dataset_name = prefix.split('_')[-1]\n",
    "        all_processed_data[dataset_name] = {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "        print(\"\\nAll datasets processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b085de",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {}\n",
    "for dataset_name, data in all_processed_data.items():\n",
    "    print(f\"\\nCreating DataLoaders for {dataset_name}:\")\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_val = data['X_val']\n",
    "    y_val = data['y_val']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "    train_loader, val_loader, test_loader = make_loaders(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        batch=64\n",
    "    )\n",
    "    loaders[dataset_name] = {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader\n",
    "    }\n",
    "    print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84269b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "budgets = (100_000, 300_000)\n",
    "seeds   = (0, 1, 2)\n",
    "val_ep  = max(10, num_epochs // 5)\n",
    "\n",
    "final_tables = {}\n",
    "final_details = {}\n",
    "\n",
    "for dataset_name in all_processed_data.keys():\n",
    "    print(f\"\\n### Param validation + training: {dataset_name}\")\n",
    "    train_loader, val_loader, test_loader = loaders[dataset_name]\n",
    "\n",
    "    # wrap into the expected structure for the runner\n",
    "    apd = {\n",
    "        dataset_name: {\n",
    "            \"X_train\": all_processed_data[dataset_name][\"X_train\"],\n",
    "            \"y_train\": all_processed_data[dataset_name][\"y_train\"],\n",
    "            \"X_val\":   all_processed_data[dataset_name][\"X_val\"],\n",
    "            \"y_val\":   all_processed_data[dataset_name][\"y_val\"],\n",
    "            \"X_test\":  all_processed_data[dataset_name][\"X_test\"],\n",
    "            \"y_test\":  all_processed_data[dataset_name][\"y_test\"],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    df_summary, details = run_experiment_block_param_validated(\n",
    "        dataset_name=dataset_name,\n",
    "        all_processed_data=apd,\n",
    "        budgets=budgets,\n",
    "        rnn_layers_options=[1, 2, 3], \n",
    "        mamba_layers_options=[1, 2, 3], \n",
    "        seeds=seeds,\n",
    "        val_epochs=val_ep,\n",
    "        tol_print=6\n",
    "    )\n",
    "    final_tables[dataset_name] = df_summary\n",
    "    final_details[dataset_name] = details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob_mamba on NYSE data\n",
    "# Build loaders\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainL, valL, testL, input_dim, L = build_seq2seq_loaders(\n",
    "    prefix=\"cleaned_NYSE\", key_in_all=\"NYSE\",\n",
    "    all_processed_data=all_processed_data, batch_size=64\n",
    ")\n",
    "\n",
    "# Build 100k model with n_state fixed to 64\n",
    "prob_mamba = build_prob_mamba_budget_fixed_n(\n",
    "    input_dim=input_dim, target=100_000, n_state=64,\n",
    "    n_layers=1, d_state=64, d_conv=4, expand=2,\n",
    "    d_feat_min=32, d_feat_max=192, d_feat_step=8, tol_frac=0.03, verbose=True\n",
    ").to(device)\n",
    "\n",
    "# Train + record (timed)\n",
    "t0 = time.perf_counter()\n",
    "hist = train_prob_mamba_with_history(\n",
    "    prob_mamba, trainL, valL, device=device, epochs=100, lr=1e-3, print_every=10\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "train_seconds = time.perf_counter() - t0\n",
    "epochs_used = 100\n",
    "n_train = len(trainL.dataset)\n",
    "total_samples = epochs_used * n_train\n",
    "train_throughput = (total_samples / train_seconds) if train_seconds > 0 else float(\"nan\")\n",
    "print(f\"[Timing • Train] {train_seconds:.2f}s ({train_seconds/60:.2f} min) | \"\n",
    "      f\"epochs={epochs_used} | samples={total_samples:,} | throughput={train_throughput:.1f} samples/s\")\n",
    "\n",
    "# Plot Val NLL vs epoch\n",
    "plot_nll_history(hist, every=10, title=\"Prob-Mamba (NYSE) • NLL\")\n",
    "\n",
    "# Evaluate\n",
    "metrics = eval_prob_rmse_qlike_laststep(prob_mamba, testL, device)\n",
    "nll = metrics.get('nll')\n",
    "nll_str = f\"{nll:.6f}\" if isinstance(nll, (int, float)) else \"N/A\"\n",
    "print(f\"[Prob-Mamba • NYSE] test NLL: {nll_str} | last-step RMSE: {metrics['rmse']:.6f} | QLIKE: {metrics['qlike']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = metrics['nll']\n",
    "nll_str = f\"{nll:.6f}\" if nll is not None else \"N/A\"\n",
    "print(f\"[Prob-Mamba • IXIC] test NLL: {nll_str} | last-step RMSE: {metrics['rmse']:.6f} | QLIKE: {metrics['qlike']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob_mamba on IXIC data\n",
    "# Build loaders\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainL, valL, testL, input_dim, L = build_seq2seq_loaders(\n",
    "    prefix=\"cleaned_IXIC\", key_in_all=\"IXIC\",\n",
    "    all_processed_data=all_processed_data, batch_size=64\n",
    ")\n",
    "\n",
    "# Build 100k model with n_state fixed to 64\n",
    "prob_mamba = build_prob_mamba_budget_fixed_n(\n",
    "    input_dim=input_dim, target=100_000, n_state=64,\n",
    "    n_layers=1, d_state=64, d_conv=4, expand=2,\n",
    "    d_feat_min=32, d_feat_max=192, d_feat_step=8, tol_frac=0.03, verbose=True\n",
    ").to(device)\n",
    "\n",
    "# Train + record (timed)\n",
    "t0 = time.perf_counter()\n",
    "hist = train_prob_mamba_with_history(\n",
    "    prob_mamba, trainL, valL, device=device, epochs=100, lr=1e-3, print_every=10\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "train_seconds = time.perf_counter() - t0\n",
    "epochs_used = 100\n",
    "n_train = len(trainL.dataset)\n",
    "total_samples = epochs_used * n_train\n",
    "train_throughput = (total_samples / train_seconds) if train_seconds > 0 else float(\"nan\")\n",
    "print(f\"[Timing • Train] {train_seconds:.2f}s ({train_seconds/60:.2f} min) | \"\n",
    "      f\"epochs={epochs_used} | samples={total_samples:,} | throughput={train_throughput:.1f} samples/s\")\n",
    "\n",
    "# Plot Val NLL vs epoch\n",
    "plot_nll_history(hist, every=10, title=\"Prob-Mamba (IXIC) • NLL\")\n",
    "\n",
    "# Evaluate\n",
    "metrics = eval_prob_rmse_qlike_laststep(prob_mamba, testL, device)\n",
    "nll = metrics.get('nll')\n",
    "nll_str = f\"{nll:.6f}\" if isinstance(nll, (int, float)) else \"N/A\"\n",
    "print(f\"[Prob-Mamba • IXIC] test NLL: {nll_str} | last-step RMSE: {metrics['rmse']:.6f} | QLIKE: {metrics['qlike']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob_mamba on BTC data\n",
    "# Build loaders\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainL, valL, testL, input_dim, L = build_seq2seq_loaders(\n",
    "    prefix=\"BTC_USDT-5m\", key_in_all=\"USDT-5m\",\n",
    "    all_processed_data=all_processed_data, batch_size=64\n",
    ")\n",
    "\n",
    "# Build 100k model with n_state fixed to 16\n",
    "prob_mamba = build_prob_mamba_budget_fixed_n(\n",
    "    input_dim=input_dim, target=100_000, n_state=16,\n",
    "    n_layers=1, d_state=16, d_conv=4, expand=2,\n",
    "    d_feat_min=16, d_feat_max=64, d_feat_step=8, tol_frac=0.03, verbose=True\n",
    ").to(device)\n",
    "\n",
    "# Train + record (timed)\n",
    "t0 = time.perf_counter()\n",
    "hist = train_prob_mamba_with_history(\n",
    "    prob_mamba, trainL, valL, device=device, epochs=100, lr=1e-3, print_every=10\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "train_seconds = time.perf_counter() - t0\n",
    "epochs_used = 100\n",
    "n_train = len(trainL.dataset)\n",
    "total_samples = epochs_used * n_train\n",
    "train_throughput = (total_samples / train_seconds) if train_seconds > 0 else float(\"nan\")\n",
    "print(f\"[Timing • Train] {train_seconds:.2f}s ({train_seconds/60:.2f} min) | \"\n",
    "      f\"epochs={epochs_used} | samples={total_samples:,} | throughput={train_throughput:.1f} samples/s\")\n",
    "\n",
    "# Plot Val NLL vs epoch\n",
    "plot_nll_history(hist, every=10, title=\"Prob-Mamba (BTC) • NLL\")\n",
    "\n",
    "# Evaluate\n",
    "metrics = eval_prob_rmse_qlike_laststep(prob_mamba, testL, device)\n",
    "nll = metrics.get('nll')\n",
    "nll_str = f\"{nll:.6f}\" if isinstance(nll, (int, float)) else \"N/A\"\n",
    "print(f\"[Prob-Mamba • BTC] test NLL: {nll_str} | last-step RMSE: {metrics['rmse']:.6f} | QLIKE: {metrics['qlike']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prob_mamba.eval()\n",
    "    metrics = eval_prob_rmse_qlike_laststep(prob_mamba, testL, device)\n",
    "nll = metrics.get('nll')\n",
    "nll_str = f\"{nll:.6f}\" if isinstance(nll, (int, float)) else \"N/A\"\n",
    "print(f\"[Prob-Mamba • BTC] test NLL: {nll_str} | last-step RMSE: {metrics['rmse']:.6f} | QLIKE: {metrics['qlike']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
