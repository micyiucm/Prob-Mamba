{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28fe6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os, re, math, warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pyarrow.feather as feather\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0c3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mom_correlation(file_path:str):\n",
    "    \"\"\"\n",
    "    This function loads the dataset and tests its momentum columns for look-ahead bias.\n",
    "    Args: file_path: the path to the feather file.\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset: \", file_path)\n",
    "    try:\n",
    "        df = pd.read_feather(file_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date']) # Ensure 'Date' is in datetime format\n",
    "        df = df.sort_values('Date').reset_index(drop=True)  # Sort by Date chronologically\n",
    "        mom_cols = [c for c in ['mom','mom1','mom2', 'mom3'] if c in df.columns] # Comprehension to extract momentum columns\n",
    "        report = {}\n",
    "        for col in mom_cols:\n",
    "            report[col] = {}\n",
    "            print(f\"\\n Testing '{col}':\")\n",
    "            s = pd.to_numeric(df[col], errors='coerce') # Convert to numeric, coerce invalid values to NaN\n",
    "            \n",
    "            for k in [1,2,3,5,10]: # Test against k-day future returns\n",
    "                future_returns = df['Price'].shift(-k)/df['Price']-1.0\n",
    "                valid_mask = s.notna() & future_returns.notna() # Both momentum and future return must be valid\n",
    "                if valid_mask.sum() < 20: # Minimum valid datapoints threshold\n",
    "                    print(\"Not enough valid data to test {col} against {k}-day future returns\")\n",
    "                    continue\n",
    "                mom_series = s[valid_mask] # Only valid momentum values\n",
    "                future_series = future_returns[valid_mask] # Only valid future returns\n",
    "                # Calculate Pearson correlation coefficient and extract correlation between mom_series and future_series\n",
    "                correlation = np.corrcoef(mom_series, future_series)[0,1] \n",
    "                report[col][f'corr_with_{k}d_future_return'] = correlation # Store in report\n",
    "                print(f\"  Correlation with {k}-day future returns: {correlation:.4f}\")\n",
    "        return report\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3d5ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset:  /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_NYSE.feather\n",
      "\n",
      " Testing 'mom':\n",
      "  Correlation with 1-day future returns: -0.9991\n",
      "  Correlation with 2-day future returns: -0.6735\n",
      "  Correlation with 3-day future returns: -0.5941\n",
      "  Correlation with 5-day future returns: -0.4331\n",
      "  Correlation with 10-day future returns: -0.3227\n",
      "\n",
      " Testing 'mom1':\n",
      "  Correlation with 1-day future returns: 0.0891\n",
      "  Correlation with 2-day future returns: -0.6736\n",
      "  Correlation with 3-day future returns: -0.4901\n",
      "  Correlation with 5-day future returns: -0.4094\n",
      "  Correlation with 10-day future returns: -0.2682\n",
      "\n",
      " Testing 'mom2':\n",
      "  Correlation with 1-day future returns: -0.0893\n",
      "  Correlation with 2-day future returns: -0.0042\n",
      "  Correlation with 3-day future returns: -0.5941\n",
      "  Correlation with 5-day future returns: -0.4665\n",
      "  Correlation with 10-day future returns: -0.3243\n",
      "\n",
      " Testing 'mom3':\n",
      "  Correlation with 1-day future returns: 0.0324\n",
      "  Correlation with 2-day future returns: -0.0414\n",
      "  Correlation with 3-day future returns: 0.0167\n",
      "  Correlation with 5-day future returns: -0.4103\n",
      "  Correlation with 10-day future returns: -0.2786\n",
      "Loading dataset:  /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_IXIC.feather\n",
      "\n",
      " Testing 'mom':\n",
      "  Correlation with 1-day future returns: -0.9992\n",
      "  Correlation with 2-day future returns: -0.6714\n",
      "  Correlation with 3-day future returns: -0.5793\n",
      "  Correlation with 5-day future returns: -0.4303\n",
      "  Correlation with 10-day future returns: -0.3214\n",
      "\n",
      " Testing 'mom1':\n",
      "  Correlation with 1-day future returns: 0.0967\n",
      "  Correlation with 2-day future returns: -0.6715\n",
      "  Correlation with 3-day future returns: -0.4903\n",
      "  Correlation with 5-day future returns: -0.3943\n",
      "  Correlation with 10-day future returns: -0.2567\n",
      "\n",
      " Testing 'mom2':\n",
      "  Correlation with 1-day future returns: -0.0533\n",
      "  Correlation with 2-day future returns: 0.0298\n",
      "  Correlation with 3-day future returns: -0.5793\n",
      "  Correlation with 5-day future returns: -0.4384\n",
      "  Correlation with 10-day future returns: -0.3061\n",
      "\n",
      " Testing 'mom3':\n",
      "  Correlation with 1-day future returns: 0.0366\n",
      "  Correlation with 2-day future returns: -0.0117\n",
      "  Correlation with 3-day future returns: 0.0466\n",
      "  Correlation with 5-day future returns: -0.3959\n",
      "  Correlation with 10-day future returns: -0.2645\n"
     ]
    }
   ],
   "source": [
    "# Perform correlation analysis on datasets\n",
    "Nyse_test = mom_correlation('/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_NYSE.feather')\n",
    "Ixic_test = mom_correlation('/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_IXIC.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f7841cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_columns(df: pd.DataFrame):\n",
    "    cols = [c for c in ['mom','mom1','mom2', 'mom3'] if c in df.columns]\n",
    "    cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    return cols\n",
    "\n",
    "def split_by_dates(df: pd.DataFrame, date_col: str, train_end: str, val_end: str):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into training, validation, and test sets based on date ranges.\n",
    "    Args:\n",
    "        df: DataFrame containing the dataset.\n",
    "        date_col: Name of the date column to sort and split data.\n",
    "        train_end: End date for the training set (inclusive).\n",
    "        val_end: End date for the validation set (inclusive).\n",
    "    \"\"\"\n",
    "    tr_end = pd.to_datetime(train_end)\n",
    "    va_end = pd.to_datetime(val_end)\n",
    "    tr_mask = df[date_col] <= tr_end\n",
    "    va_mask = (df[date_col] > tr_end) & (df[date_col] <= va_end)\n",
    "    te_mask = df[date_col] > va_end\n",
    "    return tr_mask, va_mask, te_mask\n",
    "def compute_vif(X:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Computes Variance Inflation Factor (VIF) for each column in X.\n",
    "    \"\"\"\n",
    "    cols = list(X.columns) # Convert features to list\n",
    "    if X.shape[1] < 2:\n",
    "        return pd.DataFrame({\"feature\": cols, \"VIF\": [np.nan]*len(cols)}) # VIF calculation requires at least 2 features\n",
    "    # Drop constant columns to avoid zero-variance issues\n",
    "    const_cols = X.columns[X.nunique()<=1].tolist()\n",
    "    if const_cols:\n",
    "        X = X.drop(columns=const_cols)\n",
    "        cols = list(X.columns) # Update column list\n",
    "        if X.shape[1] < 2:\n",
    "            return pd.DataFrame({\"feature\": cols, \"VIF\": [np.nan]*len(cols)}) # VIF calculation requires at least 2 features\n",
    "    Xc = sm.add_constant(X, has_constant='add') # Add constant column for intercept\n",
    "    vifs = []\n",
    "    for i, _ in enumerate(cols):\n",
    "        try:\n",
    "            vifs.append(variance_inflation_factor(Xc.values, i+1)) # i+1 to skip constant column at index 0\n",
    "        except Exception as e:\n",
    "            vifs.append(np.nan) # In case of error, append NaN\n",
    "    return pd.DataFrame({\"feature\": cols, \"VIF\": vifs}).sort_values(\n",
    "        \"VIF\", ascending=False, key = lambda s: s.fillna(-np.inf)) # Sort in descending order with NaNs at the bottom\n",
    "def vif_iterative_prune(X:pd.DataFrame, threshold: float = 10.0):\n",
    "    \"\"\"\n",
    "    Iteratively removes features with VIF above the specified threshold.\n",
    "    Args:\n",
    "        X: DataFrame containing the features.\n",
    "        threshold: VIF threshold above which features are removed.\n",
    "    Returns:\n",
    "        X_reduced: DataFrame with reduced features after pruning.\n",
    "        dropped_list: List of features that were dropped.\n",
    "        final_vif_df: DataFrame of final VIF values for remaining features.\n",
    "    \"\"\"\n",
    "    Xw = X.copy() # Work on a copy to avoid modifying original data\n",
    "    # Remove exact constants up front\n",
    "    const_cols = Xw.columns[Xw.nunique()<=1].tolist()\n",
    "    dropped = []\n",
    "    if const_cols:\n",
    "        for c in const_cols:\n",
    "            dropped.append((c, float('inf')))\n",
    "        Xw = Xw.drop(columns=const_cols)\n",
    "    while Xw.shape[1] >= 2: # Again, VIF requires at least 2 features\n",
    "        vif_df = compute_vif(Xw)\n",
    "        if vif_df.empty or vif_df[\"VIF\"].isna().all():\n",
    "            break # No valid VIFs to process\n",
    "        max_row = vif_df.iloc[0]\n",
    "        if pd.notna(max_row[\"VIF\"]) and max_row[\"VIF\"] > threshold:\n",
    "            Xw = Xw.drop(columns=[max_row[\"feature\"]]) # Drop the feature with highest VIF\n",
    "            dropped.append((max_row[\"feature\"], float(max_row[\"VIF\"]))) # Record dropped feature\n",
    "        else:\n",
    "            break # All remaining features have acceptable VIF\n",
    "    final_vif_df = compute_vif(Xw) if Xw.shape[1] >= 2 else pd.DataFrame({\"feature\": [], \"VIF\": []})\n",
    "    return Xw, dropped, final_vif_df\n",
    "def select_vif_features(\n",
    "        df: pd.DataFrame,\n",
    "        date_col: str = \"Date\",\n",
    "        include_price: bool | None = None,\n",
    "        feature_always_exclude: list | None = None,\n",
    "        drop_momentum: bool | None = None,\n",
    "        manual_drop: list | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Selects features for VIF analysis, excluding specified features.\n",
    "    Args:\n",
    "        df: DataFrame containing the dataset.\n",
    "        date_col: Name of the date column to sort data.\n",
    "        include_price: If False, excludes the price column from features.\n",
    "        feature_always_exclude: List of features to always exclude.\n",
    "        drop_momentum: If True, excludes momentum features.\n",
    "        manual_drop: List of additional features to manually exclude.\n",
    "    \"\"\"\n",
    "    # If no input provided, use global variables\n",
    "    if include_price is None:\n",
    "        include_price = globals().get(\"Include_price\", False)\n",
    "    if feature_always_exclude is None:\n",
    "        feature_always_exclude = globals().get(\"Feature_always_exclude\", [\"Date\", \"Price\", \"Name\", \"Weekday\"])\n",
    "    if drop_momentum is None:\n",
    "        drop_momentum = globals().get(\"Drop_momentum\", True)\n",
    "\n",
    "    \n",
    "    # Numeric columns\n",
    "    numeric_cols = list(df.select_dtypes(include=[np.number]).columns)\n",
    "    feature_cols = [c for c in numeric_cols if c.lower() not in excl_ci]\n",
    "    excluded_cols = sorted(set(numeric_cols) - set(feature_cols))\n",
    "    return feature_cols, excluded_cols\n",
    "\n",
    "def run_vif(\n",
    "        feather_path: str,\n",
    "        train_end: str,\n",
    "        val_end: str,\n",
    "        vif_threshold: float = 10.0,\n",
    "        date_col: str = \"Date\",\n",
    "        include_price: bool | None = None,\n",
    "        feature_always_exclude: list | None = None,\n",
    "        drop_momentum: bool | None = None,\n",
    "        manual_drop: list | None = None,\n",
    "        save_artifacts: bool | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    VIF pipeline: load data, select features, split by dates,\n",
    "    casual imputation (forward-fill), drop leading incomplete rows,\n",
    "    split by dates, run VIF on trading set and apply results to validation and test sets.\n",
    "    \"\"\"\n",
    "    # If no input provided, use global variables\n",
    "    if save_artifacts is None:\n",
    "        save_artifacts = globals().get(\"Save_artifacts\", True)\n",
    "    if include_price is None:\n",
    "        include_price = globals().get(\"Include_price\", False)\n",
    "    if feature_always_exclude is None:\n",
    "        feature_always_exclude = globals().get(\"Feature_always_exclude\", [\"Date\", \"Price\", \"Name\", \"Weekday\"])\n",
    "    if drop_momentum is None:\n",
    "        drop_momentum = globals().get(\"Drop_momentum\", True)\n",
    "\n",
    "    # Load dataset\n",
    "    if not os.path.isfile(feather_path):\n",
    "        print(f\"File not found: {feather_path}\")\n",
    "        return  None, None, None\n",
    "    print(f\"\\n VIF analysis for dataset: {feather_path}\")\n",
    "    df = pd.read_feather(feather_path)\n",
    "\n",
    "    # Ensure date column is datetime, drop duplicates and sort\n",
    "    if date_col not in df.columns:\n",
    "        raise KeyError(f\"Date column '{date_col}' not found in {feather_path}\")\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors = \"raise\")\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.sort_values(date_col).drop_duplicates(subset=[date_col]).reset_index(drop=True)\n",
    "\n",
    "    # Build exclusion set\n",
    "    exclude = set(feature_always_exclude) | {date_col}\n",
    "    if not include_price:\n",
    "        exclude.add(\"Price\")\n",
    "    if drop_momentum:\n",
    "        exclude |= set(momentum_columns(df))\n",
    "    if manual_drop:\n",
    "        exclude |= set(manual_drop)\n",
    "\n",
    "    # Numeric features only\n",
    "    numeric_cols = list(df.select_dtypes(include=[np.number]).columns)\n",
    "    feature_cols = [c for c in numeric_cols if c not in exclude]\n",
    "    excluded_cols = sorted(set(numeric_cols) - set(feature_cols))\n",
    "\n",
    "    if len(feature_cols) < 2:\n",
    "        print(\"Not enough numeric features for VIF analysis after exclusions.\")\n",
    "        return feature_cols, pd.DataFrame({\"feature\": feature_cols, \"VIF\": np.nan}), []\n",
    "\n",
    "    # Causal Imputation: forward-fill only\n",
    "    # 1. Forward-fill features only\n",
    "    df[feature_cols] = df[feature_cols].ffill()\n",
    "    # 2. Drop leading rows with any NaNs in feature columns\n",
    "    complete_mask = df[feature_cols].notna().all(axis=1) # Boolean mask for rows with all valid features\n",
    "    if not complete_mask.any():\n",
    "        print(\"No complete rows after forward-fill imputation.\")\n",
    "        return feature_cols, pd.DataFrame({\"feature\": feature_cols, \"VIF\": np.nan}), []\n",
    "    first_complete_index = complete_mask.idxmax() # First index where all features are valid\n",
    "    if isinstance(first_complete_index, (np.integer, int)):\n",
    "        df = df.loc[first_complete_index:].reset_index(drop=True) # Drop leading incomplete rows\n",
    "    else:\n",
    "        first_pos = np.argmax(complete_mask.values) # Fallback if non-integer index is returned, edge case\n",
    "        df = df.iloc[first_pos:].reset_index(drop=True) # Drop leading incomplete rows\n",
    "\n",
    "    # Splitting into training, validation and test sets based on dates\n",
    "    train_mask, val_mask, test_mask = split_by_dates(df, date_col, train_end, val_end)\n",
    "    X_train = df.loc[train_mask, feature_cols].copy()\n",
    "    X_val = df.loc[val_mask, feature_cols].copy()\n",
    "    X_test = df.loc[test_mask, feature_cols].copy()\n",
    "\n",
    "    # Ensure no NaNs remain after imputation\n",
    "    if X_train.isna().any().any() or X_val.isna().any().any() or X_test.isna().any().any():\n",
    "        X_train = X_train.dropna()\n",
    "        X_val = X_val.dropna()\n",
    "        X_test = X_test.dropna()\n",
    "    \n",
    "    # VIF on training set\n",
    "    X_train_reduced, dropped_features, final_vif_df = vif_iterative_prune(X_train, threshold=vif_threshold)\n",
    "    retained_features = list(X_train_reduced.columns)\n",
    "\n",
    "    # Apply same feature selection to validation and test sets\n",
    "    X_val_reduced = X_val[retained_features].copy()\n",
    "    X_test_reduced = X_test[retained_features].copy()\n",
    "\n",
    "    # Summary\n",
    "    print(f\"Excluded features before VIF: {excluded_cols}\")\n",
    "    print(f\"Rows in training set after imputation: {X_train.shape[0]}\")\n",
    "    print(f\"Rows in validation set after imputation: {X_val.shape[0]}\")\n",
    "    print(f\"Rows in test set after imputation: {X_test.shape[0]}\")\n",
    "    print(f\"Features retained after VIF pruning: {retained_features}\")\n",
    "    if dropped_features:\n",
    "        print(f\"Dropped features due to high VIF (> {vif_threshold}): {[f for f, v in dropped_features]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e1ed709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " VIF analysis for dataset: /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_NYSE.feather\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded features before VIF: ['Price', 'mom', 'mom1', 'mom2', 'mom3', 'weekday']\n",
      "Rows in training set after imputation: 2516\n",
      "Rows in validation set after imputation: 253\n",
      "Rows in test set after imputation: 701\n",
      "Features retained after VIF pruning: ['Vol.', 'ROC_5', 'ROC_10', 'ROC_15', 'ROC_20', 'EMA_10', 'WTI-oil', 'FTSE-F', 'HSI-F', 'Gold-F', 'NZD', 'Brent', 'DBAA', 'XAU', 'S&P-F', 'AUD', 'AMZN', 'RUSSELL-F', 'CNY', 'MSFT', 'silver-F', 'CAD', 'DAX-F', 'XOM', 'EUR', 'WFC', 'GE', 'copper-F', 'JPM', 'GAS-F', 'JPY', 'wheat-F', 'GBP', 'SSEC', 'Nikkei-F', 'CHF', 'KOSPI-F', 'AAPL', 'CAC-F', 'JNJ', 'TE3', 'TE5', 'DE1', 'DE2']\n",
      "Dropped features due to high VIF (> 10.0): ['DAAA', 'TE1', 'TE2', 'DTB4WK', 'TE6', 'DE5', 'DGS10', 'DE4', 'DTB3', 'DTB6', 'EMA_20', 'CTB6M', 'CTB3M', 'EMA_50', 'CTB1Y', 'GSPC', 'DE6', 'DJI-F', 'FCHI', 'DGS5', 'RUT', 'EMA_200', 'Dollar Index-F', 'NASDAQ-F', 'GDAXI', 'oil', 'IXIC', 'HSI', 'DJI', 'FTSE', 'XAG', 'Dollar Index']\n",
      "\n",
      " VIF analysis for dataset: /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_IXIC.feather\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/home/ubuntu/ucl_msc-dissertation_2025_michael-yiu-2/prob_mamba/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded features before VIF: ['Price', 'mom', 'mom1', 'mom2', 'mom3', 'weekday']\n",
      "Rows in training set after imputation: 2516\n",
      "Rows in validation set after imputation: 253\n",
      "Rows in test set after imputation: 701\n",
      "Features retained after VIF pruning: ['Vol.', 'ROC_5', 'ROC_10', 'ROC_15', 'ROC_20', 'EMA_10', 'WTI-oil', 'FTSE-F', 'HSI-F', 'Gold-F', 'NZD', 'Brent', 'XAU', 'AUD', 'AMZN', 'RUSSELL-F', 'CNY', 'MSFT', 'silver-F', 'CAD', 'DAX-F', 'DAAA', 'XOM', 'EUR', 'WFC', 'GE', 'copper-F', 'JPM', 'GAS-F', 'JPY', 'wheat-F', 'GBP', 'SSEC', 'Nikkei-F', 'CHF', 'KOSPI-F', 'AAPL', 'CAC-F', 'NASDAQ-F', 'JNJ', 'TE2', 'TE6', 'DE2']\n",
      "Dropped features due to high VIF (> 10.0): ['DGS10', 'DE1', 'TE1', 'TE3', 'DE5', 'TE5', 'DTB6', 'DE4', 'DTB4WK', 'DTB3', 'EMA_20', 'CTB6M', 'CTB3M', 'EMA_50', 'DE6', 'CTB1Y', 'GSPC', 'EMA_200', 'DBAA', 'S&P-F', 'FCHI', 'RUT', 'Dollar Index-F', 'DJI', 'GDAXI', 'oil', 'DGS5', 'NYSE', 'HSI', 'FTSE', 'XAG', 'Dollar Index', 'DJI-F']\n"
     ]
    }
   ],
   "source": [
    "Include_price = False\n",
    "Save_artifacts = True\n",
    "Feature_always_exclude = [\"Date\", \"Price\", \"Name\", \"weekday\"]\n",
    "Drop_momentum = True\n",
    "\n",
    "# Run VIF analysis on NYSE dataset\n",
    "Nyse_vif = run_vif(\n",
    "    feather_path='/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_NYSE.feather',\n",
    "    train_end=\"2019-12-31\",\n",
    "    val_end=\"2020-12-31\",\n",
    "    vif_threshold=10.0,\n",
    "    date_col=\"Date\"\n",
    ")\n",
    "# Run VIF analysis on IXIC dataset\n",
    "Ixic_vif = run_vif(\n",
    "    feather_path='/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_IXIC.feather',\n",
    "    train_end=\"2019-12-31\",\n",
    "    val_end=\"2020-12-31\",\n",
    "    vif_threshold=10.0,\n",
    "    date_col=\"Date\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
