{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0065cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "repo_root = Path(\"/home/ubuntu/michael/MSc-Machine-Learning-Project\")\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94508c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the equities datasets and select features\n",
    "def clean_and_select_features_equities(df: pd.DataFrame, redundant_features: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a raw equities dataframe (with date on YYYY-MM-DD format) and returns a cleaned feature set after handling look-ahead bias and multicollinearity.\n",
    "    Args:\n",
    "        df: The raw input dataframe\n",
    "        redundant_features:  list of redundant column names to drop\n",
    "    Returns:\n",
    "        A dataframe containing only the cleaned and selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by date\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # Identify and shift \"-F\" columns to avoid look-ahead bias\n",
    "    future_cols = [c for c in df.columns if '-F' in c]\n",
    "    intl_indices = ['FTSE', 'GDAXI', 'FCHI', 'HSI', 'SSEC']\n",
    "    col_to_shift = future_cols + [c for c in intl_indices if c in df.columns]\n",
    "    print(f\"\\nShifting {len(col_to_shift)} columns by +1 day\")\n",
    "\n",
    "    for col in col_to_shift:\n",
    "        df[col] = df[col].shift(1)\n",
    "\n",
    "    # Define leaky feature columns to be dropped \n",
    "    leaky_momentum = ['mom', 'mom1', 'mom2', 'mom3']\n",
    "\n",
    "     # Drop name of index (constant across all observations)\n",
    "    name= ['Name']\n",
    "\n",
    "    cols_to_drop = leaky_momentum + name + redundant_features\n",
    "    # Combine all lists of columns to drop \n",
    "    cleaned_df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    print(f\"Number of columns dropped: {len(set(cols_to_drop) & set(df.columns))}\")\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d051be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redundant features\n",
    "redundant_nyse = [\n",
    "    'TE6', 'DTB6', 'DE4', 'TE5', 'DTB4WK', 'DAAA', \n",
    "    'DGS10', 'DE5', 'DTB3', 'DE6', 'EMA_20', 'CTB6M', \n",
    "    'CTB3M', 'EMA_50', 'CTB1Y', 'TE2', 'GSPC', 'DGS5', \n",
    "    'S&P-F', 'FCHI', 'EMA_200', 'GDAXI', 'oil', 'TE3', \n",
    "    'IXIC', 'HSI', 'FTSE', 'Dollar Index', 'DJI'\n",
    "    ]\n",
    "\n",
    "redundant_ixic = [\n",
    "    'DAAA', 'DTB6', 'DTB4WK', 'DGS10', 'TE3', 'DE4', \n",
    "    'TE2', 'DE5', 'DTB3', 'DE6', 'EMA_20', 'CTB6M', \n",
    "    'CTB3M', 'EMA_50', 'CTB1Y', 'EMA_200', 'DGS5', \n",
    "    'S&P-F', 'FCHI', 'GSPC', 'GDAXI', 'oil', 'NYSE', \n",
    "    'HSI', 'FTSE', 'Dollar Index', 'TE6', 'DJI'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7045e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shifting 21 columns by +1 day\n",
      "Number of columns dropped: 34\n",
      "\n",
      "Shifting 21 columns by +1 day\n",
      "Number of columns dropped: 33\n"
     ]
    }
   ],
   "source": [
    "# Clean the data and save as feather\n",
    "df_nyse_raw = pd.read_feather(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_NYSE.feather\")\n",
    "df_nyse_cleaned = clean_and_select_features_equities(df_nyse_raw, redundant_nyse)\n",
    "df_nyse_cleaned.to_feather(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_NYSE.feather\")\n",
    "\n",
    "df_ixic_raw = pd.read_feather(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_IXIC.feather\")\n",
    "df_ixic_cleaned = clean_and_select_features_equities(df_ixic_raw, redundant_ixic)\n",
    "df_ixic_cleaned.to_feather(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_IXIC.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93763c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pre-process equities data\n",
    "def preprocess_data_equities(feather_filepath: str, train_end: str = '2019-12-31', val_end: str = '2021-12-31'):\n",
    "    \"\"\"\n",
    "    Loads data with date in YYYY-MM-DD format from an arrow file and performs a full pre-processing pipeline:\n",
    "    1. Handles missing values using forward fill (no backward fill to avoid look-ahead bias);\n",
    "    2. Drop any leading rows that remain NaNs after forward fill;\n",
    "    3. Splits data chronologically into training, validation, and test sets;\n",
    "    4. Normalise the data using StandardScaler fitted on the training set.\n",
    "    Args:\n",
    "        feather_filepath (str): The path to the Arrow file to be processed.\n",
    "        train_end (str): The end date for the training set\n",
    "        val_end (str): The end date for the validation set\n",
    "    Returns:\n",
    "        tuple: (train_df, val_df, test_df, scaler)\n",
    "    \"\"\"\n",
    "    print(f\"Beginning pre-processing of {feather_filepath}:\")\n",
    "\n",
    "    # Step 1: Load and sort the data\n",
    "    df = feather.read_feather(feather_filepath)\n",
    "    date_col = \"Date\"\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "    print(f\"Loaded data from {feather_filepath} with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "    # Step 2: Engineer target column: log-returns\n",
    "    price_col = \"Price\" if \"Price\" in df.columns else \"close\"\n",
    "    df[\"_log_price\"] = np.log(df[price_col])\n",
    "    df[\"ret_t\"] = df[\"_log_price\"].diff()\n",
    "    df[\"y_next\"]     = df[\"_log_price\"].shift(-1) - df[\"_log_price\"]\n",
    "    df = df.dropna(subset=[\"y_next\"]).reset_index(drop=True)\n",
    "\n",
    "    # Step 3: Causal imputation (forward-fill only) and drop leading incomplete rows\n",
    "    # Forward-fill all columns except data, target and log-price columns\n",
    "    protect_cols = {date_col, \"y_next\", \"_log_price\"}\n",
    "    ffill_cols = [c for c in df.columns if c not in protect_cols]\n",
    "    df.loc[:, ffill_cols] = df[ffill_cols].ffill()\n",
    "\n",
    "    # Build numeric feature set excluding target and log_price\n",
    "    numeric_feature_cols = (\n",
    "        df.select_dtypes(include=np.number).columns.difference([\"y_next\", \"_log_price\"])\n",
    "    )\n",
    "    # Drop columns that are entirely NaN (prevents false \"no complete rows\")\n",
    "    all_nan_cols = [c for c in numeric_feature_cols if df[c].isna().all()]\n",
    "    if all_nan_cols:\n",
    "        print(f\"Dropping {len(all_nan_cols)} all-NaN columns (e.g., {all_nan_cols[:5]})\")\n",
    "        df = df.drop(columns=all_nan_cols)\n",
    "        numeric_feature_cols = [c for c in numeric_feature_cols if c not in all_nan_cols]\n",
    "\n",
    "    if len(numeric_feature_cols) == 0:\n",
    "        raise ValueError(\"No numeric feature columns remain after exclusions.\")\n",
    "\n",
    "    complete_mask = df[numeric_feature_cols].notna().all(axis=1)\n",
    "    if not complete_mask.any():\n",
    "        na_counts = df[numeric_feature_cols].isna().sum().sort_values(ascending=False).head(10)\n",
    "        raise ValueError(f\"After forward-fill, no rows have complete numeric features. Top NA columns:\\n{na_counts}\")\n",
    "    first_complete_idx = complete_mask.idxmax()\n",
    "    if first_complete_idx > 0:\n",
    "        print(f\"Dropping {first_complete_idx} leading rows with unresolved NaNs.\")\n",
    "    df = df.loc[first_complete_idx:].reset_index(drop=True)\n",
    "    \n",
    "    print(\"Missing values handled.\")\n",
    "\n",
    "    # Step 4: Chronological split using the provided end dates\n",
    "    # Random splitting ignores the temporal order of the data, which is crucial for time series.\n",
    "    # Here we split the data chronologically into training, validation, and test sets.\n",
    "    train_end_date = pd.Timestamp(train_end)\n",
    "    val_end_date = pd.Timestamp(val_end)\n",
    "    train_df = df[df[date_col] <= train_end_date].copy()\n",
    "    val_df = df[(df[date_col] > train_end_date) & (df[date_col] <= val_end_date)].copy()\n",
    "    test_df = df[df[date_col] > val_end_date].copy()\n",
    "    print(\"Split data chronologically:\")\n",
    "    print(f\"Training set: {train_df.shape[0]} rows (<= {train_end_date.date()})\")\n",
    "    print(f\"Validation set: {val_df.shape[0]} rows (until {val_end_date.date()})\")\n",
    "    print(f\"Test set: {test_df.shape[0]} rows (after {val_end_date.date()})\")\n",
    "\n",
    "    # Step 5: Normalisation\n",
    "    # Exclude target and log_price from scaling\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.difference([\"y_next\", \"_log_price\"])\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df[numeric_cols])\n",
    "    print(\"Scaler fitted on training data.\")\n",
    "    # Transform the training, validation, and test sets\n",
    "    train_df[numeric_cols] = scaler.transform(train_df[numeric_cols])\n",
    "    val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
    "    test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "    return train_df, val_df, test_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68cb9d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning pre-processing of /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_IXIC.feather:\n",
      "Loaded data from /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_IXIC.feather with 3470 rows and 51 columns.\n",
      "Dropping 1 leading rows with unresolved NaNs.\n",
      "Missing values handled.\n",
      "Split data chronologically:\n",
      "Training set: 2417 rows (<= 2019-08-12)\n",
      "Validation set: 484 rows (until 2021-07-14)\n",
      "Test set: 567 rows (after 2021-07-14)\n",
      "Scaler fitted on training data.\n",
      "Processed data saved for Processed/cleaned_IXIC.feather.\n",
      "\n",
      "Beginning pre-processing of /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_NYSE.feather:\n",
      "Loaded data from /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_NYSE.feather with 3470 rows and 50 columns.\n",
      "Dropping 1 leading rows with unresolved NaNs.\n",
      "Missing values handled.\n",
      "Split data chronologically:\n",
      "Training set: 2417 rows (<= 2019-08-12)\n",
      "Validation set: 484 rows (until 2021-07-14)\n",
      "Test set: 567 rows (after 2021-07-14)\n",
      "Scaler fitted on training data.\n",
      "Processed data saved for Processed/cleaned_NYSE.feather.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Data directory\n",
    "    data_dir = \"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/\"\n",
    "\n",
    "    # Per-file cut-off dates\n",
    "    split_params = {\n",
    "        \"Processed/cleaned_IXIC.feather\":   dict(train_end=\"2019-08-12\", val_end=\"2021-07-14\"),\n",
    "        \"Processed/cleaned_NYSE.feather\":   dict(train_end=\"2019-08-12\", val_end=\"2021-07-14\")\n",
    "    }\n",
    "    # Process each file\n",
    "    for feather_file in [\"Processed/cleaned_IXIC.feather\",\n",
    "                         \"Processed/cleaned_NYSE.feather\"]:\n",
    "        feather_path = os.path.join(data_dir, feather_file)\n",
    "        try:\n",
    "            # Look up per-file dates (fallback to {} â†’ defaults)\n",
    "            params = split_params.get(feather_file, {})\n",
    "            train_df, val_df, test_df, scaler = preprocess_data_equities(\n",
    "                feather_path,\n",
    "                **params  # expands to train_end=..., val_end=...\n",
    "            )\n",
    "\n",
    "            # Save processed splits\n",
    "            stem = feather_file.replace(\".feather\", \"\")\n",
    "            train_df.to_feather(os.path.join(data_dir, f\"{stem}_train.feather\"))\n",
    "            val_df.to_feather(os.path.join(data_dir, f\"{stem}_val.feather\"))\n",
    "            test_df.to_feather(os.path.join(data_dir, f\"{stem}_test.feather\"))\n",
    "            print(f\"Processed data saved for {feather_file}.\\n\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {e}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {feather_file}: {e}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1194b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date      open      high       low     close  \\\n",
      "0 2021-01-01 00:00:00+00:00  28923.63  29017.50  28913.12  28975.65   \n",
      "1 2021-01-01 00:05:00+00:00  28975.65  28979.53  28846.28  28858.94   \n",
      "2 2021-01-01 00:10:00+00:00  28858.94  28883.20  28690.17  28752.80   \n",
      "3 2021-01-01 00:15:00+00:00  28752.80  28852.48  28720.91  28820.72   \n",
      "4 2021-01-01 00:20:00+00:00  28822.17  28846.46  28744.09  28846.46   \n",
      "\n",
      "       volume  \n",
      "0  182.889878  \n",
      "1  214.568104  \n",
      "2  442.619587  \n",
      "3  174.839779  \n",
      "4  161.316784  \n",
      "Date range\n",
      "   Start: 2021-01-01 00:00:00+00:00\n",
      "   End: 2021-12-31 23:55:00+00:00\n"
     ]
    }
   ],
   "source": [
    "df_check = pd.read_feather(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/BTC_USDT-5m.feather\")\n",
    "print(df_check.head())\n",
    "date_col = \"date\"\n",
    "if date_col in df_check.columns:\n",
    "    df_check[date_col] = pd.to_datetime(df_check[date_col])\n",
    "    print(\"Date range\")\n",
    "    print(f\"   Start: {df_check[date_col].min()}\")\n",
    "    print(f\"   End: {df_check[date_col].max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
