{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d0065cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "repo_root = Path(\"/home/ubuntu/michael/MSc-Machine-Learning-Project\")\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.feather as feather\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from mamba_ssm import Mamba\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from arch import arch_model\n",
    "from mamba_ssm import Mamba\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import Optional, Literal\n",
    "from prob_mamba.models import FeatureNet, ProbabilisticMamba \n",
    "from prob_mamba.ti_lgssm import TimeInvariantLGSSM\n",
    "from prob_mamba.utils import param_count, softplus_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94508c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the equities datasets and select features\n",
    "def clean_and_select_features(df: pd.DataFrame, redundant_features: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a raw financial dataframe and returns a cleaned feature set after handling look-ahead bias and multicollinearity.\n",
    "    Args:\n",
    "        df: The raw input dataframe\n",
    "        redundant_features:  list of redundant column names to drop\n",
    "    Returns:\n",
    "        A dataframe containing only the cleaned and selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by date\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # Identify and shift \"-F\" columns to avoid look-ahead bias\n",
    "    future_cols = [c for c in df.columns if '-F' in c]\n",
    "    intl_indices = ['FTSE', 'GDAXI', 'FCHI', 'HSI', 'SSEC']\n",
    "    col_to_shift = future_cols + [c for c in intl_indices if c in df.columns]\n",
    "    print(f\"\\nShifting {len(col_to_shift)} columns by +1 day\")\n",
    "\n",
    "    for col in col_to_shift:\n",
    "        df[col] = df[col].shift(1)\n",
    "\n",
    "    # Define leaky feature columns to be dropped \n",
    "    leaky_momentum = ['mom', 'mom1', 'mom2', 'mom3']\n",
    "\n",
    "     # Drop name of index (constant across all observations)\n",
    "    name= ['Name']\n",
    "\n",
    "    cols_to_drop = leaky_momentum + name + redundant_features\n",
    "    # Combine all lists of columns to drop \n",
    "    cleaned_df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    print(f\"Number of columns dropped: {len(set(cols_to_drop) & set(df.columns))}\")\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d051be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redundant features\n",
    "redundant_nyse = [\n",
    "    'CAC-F', 'DAX-F', 'FTSE-F', 'HSI-F', 'DJI-F', 'S&P-F',\n",
    "    'CTB1Y', 'CTB6M', 'DTB3', 'DTB4WK', 'DTB6',\n",
    "    'DBAA',\n",
    "    'EMA_20', 'EMA_50',\n",
    "    'TE2', 'TE3',\n",
    "    'oil'\n",
    "]\n",
    "redundant_ixic = [\n",
    "    'CAC-F', 'DAX-F', 'FTSE-F', 'HSI-F', 'DJI-F', 'S&P-F', 'NASDAQ-F',\n",
    "    'CTB1Y', 'CTB6M', 'DTB3', 'DTB4WK', 'DTB6',\n",
    "    'DBAA',\n",
    "    'EMA_20', 'EMA_50',\n",
    "    'TE2', 'TE3',\n",
    "    'oil',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7045e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shifting 21 columns by +1 day\n",
      "Number of columns dropped: 22\n",
      "\n",
      "Shifting 21 columns by +1 day\n",
      "Number of columns dropped: 23\n"
     ]
    }
   ],
   "source": [
    "# Clean the data and save as csv\n",
    "df_nyse_raw = pd.read_csv(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_NYSE.csv\")\n",
    "df_nyse_cleaned = clean_and_select_features(df_nyse_raw, redundant_nyse)\n",
    "df_nyse_cleaned.to_csv(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_NYSE.csv\", index=False)\n",
    "\n",
    "df_ixic_raw = pd.read_csv(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/combined_dataframe_IXIC.csv\")\n",
    "df_ixic_cleaned = clean_and_select_features(df_ixic_raw, redundant_ixic)\n",
    "df_ixic_cleaned.to_csv(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_IXIC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfad40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the bitcoin dataset and select features\n",
    "def clean_and_select_features_bitcoin(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a raw hourly Bitcoin dataframe and returns a cleaned feature set after handling look-ahead bias and multicollinearity.\n",
    "    Args:\n",
    "        df: The raw input dataframe\n",
    "    Returns:\n",
    "        A dataframe containing only the cleaned and selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by timestamp\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Remove cash-market indices\n",
    "    cash_market_indices = [\n",
    "        'Close_ibovespa', 'High_ibovespa', 'Low_ibovespa', 'Open_ibovespa', 'Volume_ibovespa',\n",
    "        'Close_ipc_mexico', 'High_ipc_mexico', 'Low_ipc_mexico', 'Open_ipc_mexico', 'Volume_ipc_mexico',\n",
    "        'Close_dax', 'High_dax', 'Low_dax', 'Open_dax', 'Volume_dax',\n",
    "        'Close_nasdaq', 'High_nasdaq', 'Low_nasdaq', 'Open_nasdaq', 'Volume_nasdaq',\n",
    "        'Close_russell_2000', 'High_russell_2000', 'Low_russell_2000', 'Open_russell_2000', 'Volume_russell_2000',\n",
    "        'Close_vix', 'High_vix', 'Low_vix', 'Open_vix', 'Volume_vix',\n",
    "        'Close_cac_40', 'High_cac_40', 'Low_cac_40', 'Open_cac_40', 'Volume_cac_40',\n",
    "        'Close_euro_stoxx_50', 'High_euro_stoxx_50', 'Low_euro_stoxx_50', 'Open_euro_stoxx_50', 'Volume_euro_stoxx_50',\n",
    "        'Close_dow_jones', 'High_dow_jones', 'Low_dow_jones', 'Open_dow_jones', 'Volume_dow_jones',\n",
    "        'Close_ftse_100', 'High_ftse_100', 'Low_ftse_100', 'Open_ftse_100', 'Volume_ftse_100',\n",
    "        'Close_sptsx', 'High_sptsx', 'Low_sptsx', 'Open_sptsx', 'Volume_sptsx',\n",
    "        'Close_sp500', 'High_sp500', 'Low_sp500', 'Open_sp500', 'Volume_sp500'    \n",
    "        ]\n",
    "    \n",
    "    # Remove OHLC for all cryptocurrencies except close and volume\n",
    "    redundant_crypto = [\n",
    "        'BNB_USDT_1h_open', 'BNB_USDT_1h_high', 'BNB_USDT_1h_low',\n",
    "        'BTC_USDT_1h_open', 'BTC_USDT_1h_high', 'BTC_USDT_1h_low',\n",
    "        'DOGE_USDT_1h_open', 'DOGE_USDT_1h_high', 'DOGE_USDT_1h_low',\n",
    "        'ETH_USDT_1h_open', 'ETH_USDT_1h_high', 'ETH_USDT_1h_low',\n",
    "        'SOL_USDT_1h_open', 'SOL_USDT_1h_high', 'SOL_USDT_1h_low',\n",
    "        'XRP_USDT_1h_open', 'XRP_USDT_1h_high', 'XRP_USDT_1h_low'\n",
    "    ]\n",
    "    # Redundant trend data\n",
    "    redundant_trend = [\n",
    "        'google_trends_buy_crypto', 'google_trends_bitcoin'\n",
    "    ]\n",
    "    cols_to_drop = cash_market_indices + redundant_crypto + redundant_trend\n",
    "\n",
    "    # Combine all lists of columns to drop \n",
    "    cleaned_df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    print(f\"Number of columns dropped: {len(set(cols_to_drop) & set(df.columns))}\")\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b5acd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped: 80\n"
     ]
    }
   ],
   "source": [
    "df_btc_raw = pd.read_csv(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Raw/Bitcoin_hourly_dataset.csv\")\n",
    "df_btc_cleaned = clean_and_select_features_bitcoin(df_btc_raw)\n",
    "df_btc_cleaned.to_csv(\"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_Bitcoin.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d1929e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert csv to arrow\n",
    "def csv_to_arrow(csv_filepath: str):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from a given path and converts it into Apache Arrow (feather) format,\n",
    "    and saves it in the same directory.\n",
    "    Args: csv_filepath (str): The path to the CSV file to be converted.\n",
    "    \"\"\"\n",
    "    # Create a new filename for the arrow file\n",
    "    arrow_filepath = csv_filepath.replace('.csv', '.arrow')\n",
    "    \n",
    "    try:\n",
    "        print(f\"Converting '{csv_filepath}' to '{arrow_filepath}'\")\n",
    "\n",
    "        # Read just the header to find the date column name\n",
    "        header_df = pd.read_csv(csv_filepath, nrows=0)\n",
    "        date_col = 'timestamp' if 'timestamp' in header_df.columns else 'Date'\n",
    "\n",
    "        # Parse_dates to ensure 'Date' column is in datetime format\n",
    "        df = pd.read_csv(csv_filepath, parse_dates= [date_col])\n",
    "\n",
    "        # Write the DataFrame to a Feather file (which uses Arrow format)\n",
    "        feather.write_feather(df, arrow_filepath)\n",
    "        print(f\"Successfully converted file\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while converting '{csv_filepath}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f8b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting '/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_IXIC.csv' to '/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_IXIC.arrow'\n",
      "Successfully converted file\n",
      "Converting '/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_NYSE.csv' to '/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_NYSE.arrow'\n",
      "Successfully converted file\n",
      "Converting '/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_Bitcoin.csv' to '/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_Bitcoin.arrow'\n",
      "Successfully converted file\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Directory where data is stored\n",
    "    data_dir = '/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/'\n",
    "\n",
    "    # List of CSV files to convert\n",
    "    csv_files = [\n",
    "        'cleaned_IXIC.csv',\n",
    "        'cleaned_NYSE.csv',\n",
    "        'cleaned_Bitcoin.csv'\n",
    "    ]\n",
    "    # Convertion\n",
    "    for csv_file in csv_files:\n",
    "        csv_filepath = os.path.join(data_dir, csv_file)\n",
    "        csv_to_arrow(csv_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c93763c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pre-process data\n",
    "def preprocess_data(arrow_filepath: str, train_end: str = '2019-12-31', val_end: str = '2021-12-31'):\n",
    "    \"\"\"\n",
    "    Loads data from an arrow file and performs a full pre-processing pipeline:\n",
    "    1. Handles missing values using forward fill (no backward fill to avoid look-ahead bias);\n",
    "    2. Drop any leading rows that remain NaNs after forward fill;\n",
    "    3. Splits data chronologically into training, validation, and test sets;\n",
    "    4. Normalise the data using StandardScaler fitted on the training set.\n",
    "    Args:\n",
    "        arrow_filepath (str): The path to the Arrow file to be processed.\n",
    "        train_end (str): The end date for the training set\n",
    "        val_end (str): The end date for the validation set\n",
    "    Returns:\n",
    "        tuple: (train_df, val_df, test_df, scaler)\n",
    "    \"\"\"\n",
    "    print(f\"Beginning pre-processing of {arrow_filepath}:\")\n",
    "\n",
    "    # Step 1: Load and sort the data\n",
    "    df = feather.read_feather(arrow_filepath)\n",
    "    date_col = \"timestamp\" if \"timestamp\" in df.columns else \"Date\"\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "    print(f\"Loaded data from {arrow_filepath} with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "    # Step 2: Engineer target column: log-returns\n",
    "    price_col = \"Price\" if \"Price\" in df.columns else \"BTC_USDT_1h_close\"\n",
    "    df[\"_log_price\"] = np.log(df[price_col])\n",
    "    df[\"ret_t\"] = df[\"_log_price\"].diff()\n",
    "    df[\"y_next\"]     = df[\"_log_price\"].shift(-1) - df[\"_log_price\"]\n",
    "    df = df.dropna(subset=[\"y_next\"]).reset_index(drop=True)\n",
    "\n",
    "    # Step 3: Causal imputation (forward-fill only) and drop leading incomplete rows\n",
    "    # Forward-fill all columns except data, target and log-price columns\n",
    "    protect_cols = {date_col, \"y_next\", \"_log_price\"}\n",
    "    ffill_cols = [c for c in df.columns if c not in protect_cols]\n",
    "    df.loc[:, ffill_cols] = df[ffill_cols].ffill()\n",
    "\n",
    "    # Build numeric feature set excluding target and log_price\n",
    "    numeric_feature_cols = (\n",
    "        df.select_dtypes(include=np.number).columns.difference([\"y_next\", \"_log_price\"])\n",
    "    )\n",
    "    # Drop columns that are entirely NaN (prevents false \"no complete rows\")\n",
    "    all_nan_cols = [c for c in numeric_feature_cols if df[c].isna().all()]\n",
    "    if all_nan_cols:\n",
    "        print(f\"Dropping {len(all_nan_cols)} all-NaN columns (e.g., {all_nan_cols[:5]})\")\n",
    "        df = df.drop(columns=all_nan_cols)\n",
    "        numeric_feature_cols = [c for c in numeric_feature_cols if c not in all_nan_cols]\n",
    "\n",
    "    if len(numeric_feature_cols) == 0:\n",
    "        raise ValueError(\"No numeric feature columns remain after exclusions.\")\n",
    "\n",
    "    complete_mask = df[numeric_feature_cols].notna().all(axis=1)\n",
    "    if not complete_mask.any():\n",
    "        na_counts = df[numeric_feature_cols].isna().sum().sort_values(ascending=False).head(10)\n",
    "        raise ValueError(f\"After forward-fill, no rows have complete numeric features. Top NA columns:\\n{na_counts}\")\n",
    "    first_complete_idx = complete_mask.idxmax()\n",
    "    if first_complete_idx > 0:\n",
    "        print(f\"Dropping {first_complete_idx} leading rows with unresolved NaNs.\")\n",
    "    df = df.loc[first_complete_idx:].reset_index(drop=True)\n",
    "    \n",
    "    print(\"Missing values handled.\")\n",
    "\n",
    "    # Step 4: Chronological split using the provided end dates\n",
    "    # Random splitting ignores the temporal order of the data, which is crucial for time series.\n",
    "    # Here we split the data chronologically into training, validation, and test sets.\n",
    "    train_end_date = pd.Timestamp(train_end)\n",
    "    val_end_date = pd.Timestamp(val_end)\n",
    "    train_df = df[df[date_col] <= train_end_date].copy()\n",
    "    val_df = df[(df[date_col] > train_end_date) & (df[date_col] <= val_end_date)].copy()\n",
    "    test_df = df[df[date_col] > val_end_date].copy()\n",
    "    print(\"Split data chronologically:\")\n",
    "    print(f\"Training set: {train_df.shape[0]} rows (<= {train_end_date.date()})\")\n",
    "    print(f\"Validation set: {val_df.shape[0]} rows (until {val_end_date.date()})\")\n",
    "    print(f\"Test set: {test_df.shape[0]} rows (after {val_end_date.date()})\")\n",
    "\n",
    "    # Step 5: Normalisation\n",
    "    # Exclude target and log_price from scaling\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.difference([\"y_next\", \"_log_price\"])\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df[numeric_cols])\n",
    "    print(\"Scaler fitted on training data.\")\n",
    "    # Transform the training, validation, and test sets\n",
    "    train_df[numeric_cols] = scaler.transform(train_df[numeric_cols])\n",
    "    val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
    "    test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "    return train_df, val_df, test_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68cb9d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning pre-processing of /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_IXIC.arrow:\n",
      "Loaded data from /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_IXIC.arrow with 3470 rows and 61 columns.\n",
      "Dropping 1 leading rows with unresolved NaNs.\n",
      "Missing values handled.\n",
      "Split data chronologically:\n",
      "Training set: 2417 rows (<= 2019-08-12)\n",
      "Validation set: 484 rows (until 2021-07-14)\n",
      "Test set: 567 rows (after 2021-07-14)\n",
      "Scaler fitted on training data.\n",
      "Processed data saved for cleaned_IXIC.arrow.\n",
      "\n",
      "Beginning pre-processing of /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_NYSE.arrow:\n",
      "Loaded data from /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_NYSE.arrow with 3470 rows and 62 columns.\n",
      "Dropping 1 leading rows with unresolved NaNs.\n",
      "Missing values handled.\n",
      "Split data chronologically:\n",
      "Training set: 2515 rows (<= 2019-12-31)\n",
      "Validation set: 505 rows (until 2021-12-31)\n",
      "Test set: 448 rows (after 2021-12-31)\n",
      "Scaler fitted on training data.\n",
      "Processed data saved for cleaned_NYSE.arrow.\n",
      "\n",
      "Beginning pre-processing of /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_Bitcoin.arrow:\n",
      "Loaded data from /home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/cleaned_Bitcoin.arrow with 17516 rows and 19 columns.\n",
      "Dropping 1 leading rows with unresolved NaNs.\n",
      "Missing values handled.\n",
      "Split data chronologically:\n",
      "Training set: 12264 rows (<= 2024-09-01)\n",
      "Validation set: 2664 rows (until 2024-12-21)\n",
      "Test set: 2586 rows (after 2024-12-21)\n",
      "Scaler fitted on training data.\n",
      "Processed data saved for cleaned_Bitcoin.arrow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Data directory\n",
    "    data_dir = \"/home/ubuntu/michael/MSc-Machine-Learning-Project/Datasets/Processed/\"\n",
    "\n",
    "    # Per-file cut-off dates\n",
    "    split_params = {\n",
    "        \"cleaned_IXIC.arrow\":   dict(train_end=\"2019-08-12\", val_end=\"2021-07-14\"),\n",
    "        \"cleaned_NYSE.arrow\":   dict(train_end=\"2019-12-31\", val_end=\"2021-12-31\"),\n",
    "        \"cleaned_Bitcoin.arrow\": dict(train_end=\"2024-09-01\", val_end=\"2024-12-21\"),\n",
    "        # If you omit a file or set either value to None, preprocess_data uses its defaults.\n",
    "    }\n",
    "    # Process each file\n",
    "    for arrow_file in [\"cleaned_IXIC.arrow\",\n",
    "                       \"cleaned_NYSE.arrow\",\n",
    "                       \"cleaned_Bitcoin.arrow\"]:\n",
    "        arrow_path = os.path.join(data_dir, arrow_file)\n",
    "        try:\n",
    "            # Look up per-file dates (fallback to {} â†’ defaults)\n",
    "            params = split_params.get(arrow_file, {})\n",
    "            train_df, val_df, test_df, scaler = preprocess_data(\n",
    "                arrow_path,\n",
    "                **params  # expands to train_end=..., val_end=...\n",
    "            )\n",
    "\n",
    "            # Save processed splits\n",
    "            stem = arrow_file.replace(\".arrow\", \"\")\n",
    "            train_df.to_feather(os.path.join(data_dir, f\"{stem}_train.arrow\"))\n",
    "            val_df.to_feather(os.path.join(data_dir, f\"{stem}_val.arrow\"))\n",
    "            test_df.to_feather(os.path.join(data_dir, f\"{stem}_test.arrow\"))\n",
    "            print(f\"Processed data saved for {arrow_file}.\\n\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {e}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {arrow_file}: {e}\\n\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
